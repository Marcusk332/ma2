{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part II: BERT\n",
    "\n",
    "Please see the description of the assignment in the README file (section 2) <br>\n",
    "**Guide notebook**: [guides/bert_guide.ipynb](guides/bert_guide.ipynb)\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "* Note that you should report results using a classification report. \n",
    "\n",
    "* Also, remember to include some reflections on your results: how do they compare with the results from Part I, BoW? Are there any hyperparameters that are particularly important?\n",
    "\n",
    "* You should follow the steps given in the `bert_guide` notebook\n",
    "\n",
    "* **Optionally**, you can fine-tune a pre-trained BERT model to classify news articles as is done in [guides/bert_guide_finetuning.ipybb](guides/bert_guide_finetuning.ipybb), the same task as in part 1. As this requires more computational resources, this part is optional. If you do decide to complete this part, you will need to use a GPU (e.g., Google Colab) to train the model. (For reference, training on a 2020 Macbook Pro with 16GB RAM and a M1 chip results in an out-of-memory error). Therefore, we suggest that you use Google Colab or another cloud-based service with a GPU. You can easily upload the `bert_guide_finetuning.ipynb` notebook to Google Colab and run it there.\n",
    "\n",
    "<br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the project\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "\n",
    "We can load this data directly from [Hugging Face Datasets](https://huggingface.co/docs/datasets/) - The HuggingFace Hub- into a Pandas DataFrame. Pretty neat!\n",
    "\n",
    "**Note**: This cell will download the dataset and keep it in memory. If you run this cell multiple times, it will download the dataset multiple times.\n",
    "\n",
    "You are welcome to increase the `frac` parameter to load more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 1 # percent as whole number\n",
    "TEST_SIZE = 10 # percent as whole number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 24000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 7600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_news_train = load_dataset(\"fancyzhx/ag_news\", split=\"train[:20%]\")  # 20% of the training data\n",
    "ag_news_test = load_dataset(\"fancyzhx/ag_news\", split=\"test\")  # full test data\n",
    "\n",
    "ag_news = DatasetDict({\n",
    "    \"train\": ag_news_train,\n",
    "    \"test\": ag_news_test\n",
    "})\n",
    "\n",
    "ag_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "embedder = pipeline(\n",
    "    model=\"answerdotai/ModernBERT-base\",      # model used for embedding\n",
    "    tokenizer=\"answerdotai/ModernBERT-base\",  # tokenizer used for embedding\n",
    "    task=\"feature-extraction\",                # feature extraction task (returns embeddings)\n",
    "    device=0                                  # use GPU 0 if available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(data):\n",
    "    \"\"\" Extract the [CLS] embedding for each text. \"\"\"\n",
    "    embeddings = embedder(data[\"text\"])  # Full token embeddings\n",
    "    cls_embeddings = [e[0][0] for e in embeddings]  # Extract first token ([CLS])\n",
    "    return {\"embeddings\": cls_embeddings}\n",
    "\n",
    "ag_news = ag_news.map(get_embeddings, batched=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'embeddings'],\n",
       "        num_rows: 24000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'embeddings'],\n",
       "        num_rows: 7600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (24000, 768), y_train shape: (24000,)\n",
      "X_test shape: (7600, 768), y_test shape: (7600,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      6195\n",
      "           1       0.97      0.98      0.97      5856\n",
      "           2       0.87      0.87      0.87      5601\n",
      "           3       0.89      0.89      0.89      6348\n",
      "\n",
      "    accuracy                           0.91     24000\n",
      "   macro avg       0.91      0.91      0.91     24000\n",
      "weighted avg       0.91      0.91      0.91     24000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(ag_news[\"train\"][\"embeddings\"])  # Feature embeddings\n",
    "y_train = np.array(ag_news[\"train\"][\"label\"])       # Labels\n",
    "\n",
    "X_test = np.array(ag_news[\"test\"][\"embeddings\"])\n",
    "y_test = np.array(ag_news[\"test\"][\"label\"])\n",
    "\n",
    "# Check shapes\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "# Train a logistic regression model\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Print classification report\n",
    "y_pred_train = lr.predict(X_train)\n",
    "print(classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.88      1900\n",
      "           1       0.96      0.96      0.96      1900\n",
      "           2       0.84      0.81      0.82      1900\n",
      "           3       0.82      0.87      0.84      1900\n",
      "\n",
      "    accuracy                           0.88      7600\n",
      "   macro avg       0.88      0.88      0.88      7600\n",
      "weighted avg       0.88      0.88      0.88      7600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 1, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.89      1900\n",
      "           1       0.95      0.96      0.96      1900\n",
      "           2       0.85      0.81      0.83      1900\n",
      "           3       0.82      0.88      0.85      1900\n",
      "\n",
      "    accuracy                           0.88      7600\n",
      "   macro avg       0.88      0.88      0.88      7600\n",
      "weighted avg       0.88      0.88      0.88      7600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "        \n",
    "            'C': [0.01, 0.1, 1, 10],\n",
    "            'solver': ['liblinear'],             # saga supports l1 and l2 penalties\n",
    "            'max_iter': [1000, 5000, 10000],\n",
    "            'penalty': ['l1', 'l2']\n",
    "        \n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(LogisticRegression(random_state=42), param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection on Performance and Hyperparameter Choices\n",
    "\n",
    "The performance of the system demonstrates the effectiveness of using BERT embeddings combined with a Logistic Regression classifier for text classification. The classification report indicates that the model achieves reasonable accuracy on both the training and test datasets, suggesting that the embeddings capture meaningful features from the text. The tuning of the model performed slightly better compared to the default hyperparameters, as evidenced by the improved classification report metrics.\n",
    "\n",
    "The best hyperparameters (`C=1`, `penalty='l1'`, `solver='liblinear'`, `max_iter=1000`) were selected based on grid search with cross-validation. \n",
    "\n",
    "#### Limitations\n",
    "The processing time for embedding extraction and hyperparameter tuning was a restriction. To address this, the dataset size and hyperparameter grid were constrained. Future work could explore distributed computing or cloud-based solutions (e.g., Google Colab with GPU) to enable a more comprehensive search and faster processing.\n",
    "\n",
    "Overall, the system performs well given the constraints, but there is room for improvement with additional computational resources and a broader hyperparameter search.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
