{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part I: Bag-of-Words Model\n",
    "\n",
    "Please see the description of the assignment in the README file (section 1) <br>\n",
    "**Guide notebook**: [guides/bow_guide.ipynb](guides/bow_guide.ipynb)\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "* Note that you should report results using a classification report. \n",
    "\n",
    "* Also, remember to include some reflections on your results: Are there any hyperparameters that are particularly important?\n",
    "\n",
    "* You should follow the steps given in the `bow_guide` notebook\n",
    "\n",
    "<br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the project\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "\n",
    "We can load this data directly from [Hugging Face Datasets](https://huggingface.co/docs/datasets/) - The HuggingFace Hub- into a Pandas DataFrame. Pretty neat!\n",
    "\n",
    "**Note**: This cell will download the dataset and keep it in memory. If you run this cell multiple times, it will download the dataset multiple times.\n",
    "\n",
    "You are welcome to increase the `frac` parameter to load more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 2) (7600, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "\n",
    "train = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"train\"])\n",
    "test = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"test\"])\n",
    "\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1200, 2), (760, 2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label_map = {\n",
    "    0: 'World',\n",
    "    1: 'Sports',\n",
    "    2: 'Business',\n",
    "    3: 'Sci/Tech'\n",
    "}\n",
    "\n",
    "def preprocess(df: pd.DataFrame, frac : float = 1e-2, label_map : dict[int, str] = label_map, seed : int = 42) -> pd.DataFrame:\n",
    "    \"\"\" Preprocess the dataset \n",
    "\n",
    "    Operations:\n",
    "    - Map the label to the corresponding category\n",
    "    - Filter out the labels not in the label_map\n",
    "    - Sample a fraction of the dataset (stratified by label)\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The dataset to preprocess\n",
    "    - frac (float): The fraction of the dataset to sample in each category\n",
    "    - label_map (dict): A mapping of the original label to the new label\n",
    "    - seed (int): The random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The preprocessed dataset\n",
    "    \"\"\"\n",
    "\n",
    "    return  (\n",
    "        df\n",
    "        .assign(label=lambda x: x['label'].map(label_map))\n",
    "        [lambda df: df['label'].isin(label_map.values())]\n",
    "        .groupby('label')[[\"text\", \"label\"]]\n",
    "        .apply(lambda x: x.sample(frac=frac, random_state=seed))\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "    )\n",
    "\n",
    "train_df = preprocess(train, frac=0.01)\n",
    "test_df = preprocess(test, frac=0.1)\n",
    "\n",
    "# clear up some memory by deleting the original dataframes\n",
    "del train\n",
    "del test\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slip the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960,) (240,) (960,) (240,)\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    \n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test\n",
    "\n",
    ") = train_test_split(train_df[\"text\"], train_df[\"label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] END ................C=0.1, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ................C=0.1, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ................C=0.1, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ....................C=0.1, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ....................C=0.1, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ....................C=0.1, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ................C=0.1, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END ................C=0.1, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END ................C=0.1, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END ....................C=0.1, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ....................C=0.1, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ....................C=0.1, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ......................C=1, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ......................C=1, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ......................C=1, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ..................C=1, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..................C=1, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..................C=1, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END ......................C=1, penalty=l2, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ......................C=1, penalty=l2, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ......................C=1, penalty=l2, solver=lbfgs; total time=   0.1s\n",
      "[CV] END .................C=10, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END .................C=10, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END .................C=10, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END .....................C=10, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....................C=10, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....................C=10, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .................C=10, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END .................C=10, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END .................C=10, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END .....................C=10, penalty=l2, solver=lbfgs; total time=   0.1s\n",
      "[CV] END .....................C=10, penalty=l2, solver=lbfgs; total time=   0.1s\n",
      "[CV] END .....................C=10, penalty=l2, solver=lbfgs; total time=   0.1s\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "[CV] END .......alpha=0.01, fit_prior=True, force_alpha=True; total time=   0.0s\n",
      "[CV] END .......alpha=0.01, fit_prior=True, force_alpha=True; total time=   0.0s\n",
      "[CV] END .......alpha=0.01, fit_prior=True, force_alpha=True; total time=   0.0s\n",
      "[CV] END ......alpha=0.01, fit_prior=True, force_alpha=False; total time=   0.0s\n",
      "[CV] END ......alpha=0.01, fit_prior=True, force_alpha=False; total time=   0.0s\n",
      "[CV] END ......alpha=0.01, fit_prior=True, force_alpha=False; total time=   0.0s\n",
      "[CV] END ......alpha=0.01, fit_prior=False, force_alpha=True; total time=   0.0s\n",
      "[CV] END ......alpha=0.01, fit_prior=False, force_alpha=True; total time=   0.0s\n",
      "[CV] END ......alpha=0.01, fit_prior=False, force_alpha=True; total time=   0.0s\n",
      "[CV] END .....alpha=0.01, fit_prior=False, force_alpha=False; total time=   0.0s\n",
      "[CV] END .....alpha=0.01, fit_prior=False, force_alpha=False; total time=   0.0s\n",
      "[CV] END .....alpha=0.01, fit_prior=False, force_alpha=False; total time=   0.0s\n",
      "[CV] END ........alpha=0.1, fit_prior=True, force_alpha=True; total time=   0.0s\n",
      "[CV] END ........alpha=0.1, fit_prior=True, force_alpha=True; total time=   0.0s\n",
      "[CV] END ........alpha=0.1, fit_prior=True, force_alpha=True; total time=   0.0s\n",
      "[CV] END .......alpha=0.1, fit_prior=True, force_alpha=False; total time=   0.0s\n",
      "[CV] END .......alpha=0.1, fit_prior=True, force_alpha=False; total time=   0.0s\n",
      "[CV] END .......alpha=0.1, fit_prior=True, force_alpha=False; total time=   0.0s\n",
      "[CV] END .......alpha=0.1, fit_prior=False, force_alpha=True; total time=   0.0s\n",
      "[CV] END .......alpha=0.1, fit_prior=False, force_alpha=True; total time=   0.0s\n",
      "[CV] END .......alpha=0.1, fit_prior=False, force_alpha=True; total time=   0.0s\n",
      "[CV] END ......alpha=0.1, fit_prior=False, force_alpha=False; total time=   0.0s\n",
      "[CV] END ......alpha=0.1, fit_prior=False, force_alpha=False; total time=   0.0s\n",
      "[CV] END ......alpha=0.1, fit_prior=False, force_alpha=False; total time=   0.0s\n",
      "[CV] END ..........alpha=1, fit_prior=True, force_alpha=True; total time=   0.0s\n",
      "[CV] END ..........alpha=1, fit_prior=True, force_alpha=True; total time=   0.0s\n",
      "[CV] END ..........alpha=1, fit_prior=True, force_alpha=True; total time=   0.0s\n",
      "[CV] END .........alpha=1, fit_prior=True, force_alpha=False; total time=   0.0s\n",
      "[CV] END .........alpha=1, fit_prior=True, force_alpha=False; total time=   0.0s\n",
      "[CV] END .........alpha=1, fit_prior=True, force_alpha=False; total time=   0.0s\n",
      "[CV] END .........alpha=1, fit_prior=False, force_alpha=True; total time=   0.0s\n",
      "[CV] END .........alpha=1, fit_prior=False, force_alpha=True; total time=   0.0s\n",
      "[CV] END .........alpha=1, fit_prior=False, force_alpha=True; total time=   0.0s\n",
      "[CV] END ........alpha=1, fit_prior=False, force_alpha=False; total time=   0.0s\n",
      "[CV] END ........alpha=1, fit_prior=False, force_alpha=False; total time=   0.0s\n",
      "[CV] END ........alpha=1, fit_prior=False, force_alpha=False; total time=   0.0s\n",
      "[CV] END ..........alpha=2, fit_prior=True, force_alpha=True; total time=   0.0s\n",
      "[CV] END ..........alpha=2, fit_prior=True, force_alpha=True; total time=   0.0s\n",
      "[CV] END ..........alpha=2, fit_prior=True, force_alpha=True; total time=   0.0s\n",
      "[CV] END .........alpha=2, fit_prior=True, force_alpha=False; total time=   0.0s\n",
      "[CV] END .........alpha=2, fit_prior=True, force_alpha=False; total time=   0.0s\n",
      "[CV] END .........alpha=2, fit_prior=True, force_alpha=False; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcuskrarup/anaconda3/envs/aiml25-ma2/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
      "9 fits failed out of a total of 36.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcuskrarup/anaconda3/envs/aiml25-ma2/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcuskrarup/anaconda3/envs/aiml25-ma2/lib/python3.11/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/marcuskrarup/anaconda3/envs/aiml25-ma2/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/marcuskrarup/anaconda3/envs/aiml25-ma2/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/marcuskrarup/anaconda3/envs/aiml25-ma2/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.47916667        nan 0.79375    0.79583333 0.73229167        nan\n",
      " 0.79270833 0.79270833 0.74270833        nan 0.79583333 0.790625  ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .........alpha=2, fit_prior=False, force_alpha=True; total time=   0.0s\n",
      "[CV] END .........alpha=2, fit_prior=False, force_alpha=True; total time=   0.0s\n",
      "[CV] END .........alpha=2, fit_prior=False, force_alpha=True; total time=   0.0s\n",
      "[CV] END ........alpha=2, fit_prior=False, force_alpha=False; total time=   0.0s\n",
      "[CV] END ........alpha=2, fit_prior=False, force_alpha=False; total time=   0.0s\n",
      "[CV] END ........alpha=2, fit_prior=False, force_alpha=False; total time=   0.0s\n",
      "[CV] END ..........alpha=3, fit_prior=True, force_alpha=True; total time=   0.0s\n",
      "[CV] END ..........alpha=3, fit_prior=True, force_alpha=True; total time=   0.0s\n",
      "[CV] END ..........alpha=3, fit_prior=True, force_alpha=True; total time=   0.0s\n",
      "[CV] END .........alpha=3, fit_prior=True, force_alpha=False; total time=   0.0s\n",
      "[CV] END .........alpha=3, fit_prior=True, force_alpha=False; total time=   0.0s\n",
      "[CV] END .........alpha=3, fit_prior=True, force_alpha=False; total time=   0.0s\n",
      "[CV] END .........alpha=3, fit_prior=False, force_alpha=True; total time=   0.0s\n",
      "[CV] END .........alpha=3, fit_prior=False, force_alpha=True; total time=   0.0s\n",
      "[CV] END .........alpha=3, fit_prior=False, force_alpha=True; total time=   0.0s\n",
      "[CV] END ........alpha=3, fit_prior=False, force_alpha=False; total time=   0.0s\n",
      "[CV] END ........alpha=3, fit_prior=False, force_alpha=False; total time=   0.0s\n",
      "[CV] END ........alpha=3, fit_prior=False, force_alpha=False; total time=   0.0s\n",
      "[CV] END .........alpha=10, fit_prior=True, force_alpha=True; total time=   0.0s\n",
      "[CV] END .........alpha=10, fit_prior=True, force_alpha=True; total time=   0.0s\n",
      "[CV] END .........alpha=10, fit_prior=True, force_alpha=True; total time=   0.0s\n",
      "[CV] END ........alpha=10, fit_prior=True, force_alpha=False; total time=   0.0s\n",
      "[CV] END ........alpha=10, fit_prior=True, force_alpha=False; total time=   0.0s\n",
      "[CV] END ........alpha=10, fit_prior=True, force_alpha=False; total time=   0.0s\n",
      "[CV] END ........alpha=10, fit_prior=False, force_alpha=True; total time=   0.0s\n",
      "[CV] END ........alpha=10, fit_prior=False, force_alpha=True; total time=   0.0s\n",
      "[CV] END ........alpha=10, fit_prior=False, force_alpha=True; total time=   0.0s\n",
      "[CV] END .......alpha=10, fit_prior=False, force_alpha=False; total time=   0.0s\n",
      "[CV] END .......alpha=10, fit_prior=False, force_alpha=False; total time=   0.0s\n",
      "[CV] END .......alpha=10, fit_prior=False, force_alpha=False; total time=   0.0s\n",
      "\n",
      "--- Model Evaluation Using Your Code from Section 2.6 ---\n",
      "\n",
      "Original Logistic Regression Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Business       0.78      0.68      0.72        62\n",
      "    Sci/Tech       0.77      0.67      0.71        60\n",
      "      Sports       0.80      0.88      0.84        60\n",
      "       World       0.74      0.86      0.79        58\n",
      "\n",
      "    accuracy                           0.77       240\n",
      "   macro avg       0.77      0.77      0.77       240\n",
      "weighted avg       0.77      0.77      0.77       240\n",
      "\n",
      "Best Parameters for Logistic Regression: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "\n",
      "Tuned Logistic Regression Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Business       0.78      0.65      0.71        62\n",
      "    Sci/Tech       0.72      0.65      0.68        60\n",
      "      Sports       0.76      0.87      0.81        60\n",
      "       World       0.76      0.88      0.82        58\n",
      "\n",
      "    accuracy                           0.76       240\n",
      "   macro avg       0.76      0.76      0.76       240\n",
      "weighted avg       0.76      0.76      0.75       240\n",
      "\n",
      "\n",
      "Original Naive Bayes Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Business       0.79      0.84      0.81        62\n",
      "    Sci/Tech       0.88      0.63      0.74        60\n",
      "      Sports       0.87      0.88      0.88        60\n",
      "       World       0.74      0.90      0.81        58\n",
      "\n",
      "    accuracy                           0.81       240\n",
      "   macro avg       0.82      0.81      0.81       240\n",
      "weighted avg       0.82      0.81      0.81       240\n",
      "\n",
      "Best Parameters for Naive Bayes: {'alpha': 1, 'fit_prior': False, 'force_alpha': True}\n",
      "\n",
      "Tuned Naive Bayes Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Business       0.79      0.84      0.81        62\n",
      "    Sci/Tech       0.88      0.63      0.74        60\n",
      "      Sports       0.87      0.88      0.88        60\n",
      "       World       0.74      0.90      0.81        58\n",
      "\n",
      "    accuracy                           0.81       240\n",
      "   macro avg       0.82      0.81      0.81       240\n",
      "weighted avg       0.82      0.81      0.81       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Initialize the CountVectorizer (Bag of Words model)\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=10000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Logistic Regression Model\n",
    "model_lr = LogisticRegression(max_iter=200)\n",
    "model_lr.fit(X_train_vec, y_train)\n",
    "y_pred_lr = model_lr.predict(X_test_vec)\n",
    "\n",
    "# Naive Bayes Model\n",
    "model_nb = MultinomialNB()\n",
    "model_nb.fit(X_train_vec, y_train)\n",
    "y_pred_nb = model_nb.predict(X_test_vec)\n",
    "\n",
    "# Hyperparameter Tuning for Logistic Regression\n",
    "param_grid_lr = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "    , 'penalty': ['l1', 'l2']\n",
    "}\n",
    "grid_search_lr = GridSearchCV(LogisticRegression(max_iter=200), param_grid_lr, cv=3, verbose=2)\n",
    "grid_search_lr.fit(X_train_vec, y_train)\n",
    "\n",
    "# Hyperparameter Tuning for Naive Bayes\n",
    "param_grid_nb = {\n",
    "    'alpha': [0.01, 0.1, 1, 2, 3, 10],\n",
    "    'fit_prior': [True, False]\n",
    "    , 'force_alpha': [True, False]\n",
    "}\n",
    "grid_search_nb = GridSearchCV(MultinomialNB(), param_grid_nb, cv=3, verbose=2)\n",
    "grid_search_nb.fit(X_train_vec, y_train)\n",
    "\n",
    "# Evaluation \n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "\n",
    "# Logistic Regression - Original\n",
    "print(\"\\nOriginal Logistic Regression Model Performance:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# Logistic Regression - Tuned\n",
    "print('Best Parameters for Logistic Regression:', grid_search_lr.best_params_)\n",
    "y_pred_tuned_lr = grid_search_lr.best_estimator_.predict(X_test_vec)\n",
    "print(\"\\nTuned Logistic Regression Model Performance:\")\n",
    "print(classification_report(y_test, y_pred_tuned_lr))\n",
    "\n",
    "# Naive Bayes - Original\n",
    "print(\"\\nOriginal Naive Bayes Model Performance:\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "\n",
    "# Naive Bayes - Tuned\n",
    "print('Best Parameters for Naive Bayes:', grid_search_nb.best_params_)\n",
    "y_pred_tuned_nb = grid_search_nb.best_estimator_.predict(X_test_vec)\n",
    "print(\"\\nTuned Naive Bayes Model Performance:\")\n",
    "print(classification_report(y_test, y_pred_tuned_nb))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bag-of-Words model was tested with Logistic Regression and Naive Bayes classifiers.\n",
    "\n",
    "The Logistic Regression model performed well with some very minor improvement after hyperparameter tuning.\n",
    "The Naive Bayes model also performed reasonably well. there was experimented with 3 different hyperparameters none of them gave the model better predicting power.\n",
    "\n",
    "Further improvements could include experimenting with TF-IDF, trying SVM, or even using neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
